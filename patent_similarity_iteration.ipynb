{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> :warning: **If you are using macOS**: This notebook can not be trained on\n",
    "> macOS. If you try to run all, the notebook will automatically stop\n",
    "> execution at the training stage, and transfer execution to a remote Kaggle\n",
    "> GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from platform import system\n",
    "import warnings\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from fastai.imports import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from fastkaggle import *\n",
    "except ModuleNotFoundError:\n",
    "    ! pip install -Uq fastkaggle\n",
    "    from fastkaggle import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "environment = system()\n",
    "environment, iskaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = setup_comp('us-patent-phrase-to-phrase-matching')\n",
    "if not iskaggle:\n",
    "    ZipFile(f'{data_path}.zip').extractall(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explore Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path / 'train.csv')\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.describe(include='object')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Targets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The vast majority of targets are unique. Most targets also contain very few\n",
    "words. The least frequent targets tend to contain more than one word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Anchors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['anchor'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many targets than anchors. The length of each anchor varies quite a\n",
    " bit too, though they tend to be between 2-4 words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['context'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first letter in each context code references the section under which the\n",
    "patent was filed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['score'].hist();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most patents seem to be somewhat similar or not very similar.\n",
    "\n",
    "Below are the data points that have a score of 1.0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df[train_df['score']==1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be seen that the anchors and targets that have scored 1.0 are minor\n",
    "rewords of each other. Each patent's context doesn't seem to be playing a\n",
    "significant role."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_path/'test.csv')\n",
    "len(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.describe(include='object')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Context Section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first letter of each patent's context code refers to the section the\n",
    "patent was filed under."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['context'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separating this letter into its own column may help\n",
    "with performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['section'] = train_df['context'].str[0]\n",
    "train_df['section'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization and Numericalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Disabling warnings since Huggingface outputs them unnecessarily.\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll initially use deberta-v3-small for experimentation. The larger version\n",
    "can be used at the end."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'microsoft/deberta-v3-small'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll need to iterate on how to combine the anchors, targets, and contexts\n",
    "together since there is not much research on this.\n",
    "\n",
    "To begin with, I'll combine each data point's anchor, target, and context\n",
    "into a single string. A special token will be used to separate these sections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "separator = tokenizer.sep_token; separator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['inputs'] = train_df['context'] + separator + train_df['anchor'] + \\\n",
    "                     separator + train_df['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Better performance is obtained when Pandas DataFrames are converted into\n",
    "HuggingFace Datasets.\n",
    "\n",
    "The `score` column will also be renamed to `label` in the training set, since\n",
    "that\n",
    " is what\n",
    "HuggingFace expects."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df).rename_column('score', 'label')\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_function(document):\n",
    "    return tokenizer(document['inputs'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenize_function(train_ds[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`1` and `2` are special tokens. `1` represents the start of a document and\n",
    "`2` represents the separator token I used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = ('anchor', 'target', 'context',)\n",
    "tok_train_ds = train_ds.map(tokenize_function, batched=True,\n",
    "                            remove_columns=inputs+('inputs', 'id', 'section'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tok_train_ds[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll create a validation set that contains anchors that are not present in\n",
    "the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors = train_df.anchor.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(anchors)\n",
    "anchors[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_ratio = 0.25\n",
    "valid_set_size = int(len(anchors) * split_ratio)\n",
    "valid_set_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_anchors = anchors[:valid_set_size]\n",
    "valid_anchors[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_documents = np.isin(train_df['anchor'], valid_anchors)\n",
    "indicies = np.arange(len(train_df))\n",
    "valid_indices = indicies[valid_documents]\n",
    "train_indicies = indicies[~valid_documents]\n",
    "len(valid_indices), len(train_indicies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If error with this dictionary, change 'valid' to 'train'.\n",
    "ds_dict = DatasetDict({\n",
    "    'train': tok_train_ds.select(train_indicies),\n",
    "    'valid': tok_train_ds.select(valid_indices)\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use Kaggle remotely if on Mac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if environment == 'Darwin':\n",
    "    # Create metadata file.\n",
    "    nb_meta(\n",
    "        user='forbo7',\n",
    "        id='forbo7/push-patent-similarity-iteration',\n",
    "        title='[PUSH] Patent Similarity Iteration',\n",
    "        file='patent_similarity_iteration.ipynb',\n",
    "        competition='us-patent-phrase-to-phrase-matching',\n",
    "        private=True,\n",
    "        gpu=True\n",
    "    )\n",
    "\n",
    "    # Push to Kaggle.\n",
    "    push_notebook(\n",
    "        user='forbo7',\n",
    "        id='forbo7/push-patent-similarity-iteration',\n",
    "        title='[PUSH] Patent Similarity Iteration',\n",
    "        file='patent_similarity_iteration.ipynb',\n",
    "        competition='us-patent-phrase-to-phrase-matching',\n",
    "        private=True,\n",
    "        gpu=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Warning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return [\n",
    "            \"The training portion of the this notebook can not be run on a \"\n",
    "            \"Mac. Instead, the notebook is now being executed remotely on \"\n",
    "            \"Kaggle. This notebook will not execute further locally.\"\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if environment == 'Darwin':\n",
    "    raise StopExecution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metric Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pearson_correlation(valid_prediction):\n",
    "    return {'pearson': np.corrcoef(*valid_prediction)[0][1]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 8e-5\n",
    "batch_size = 128\n",
    "weight_decay = 0.01\n",
    "epochs = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Arguments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arguments = TrainingArguments(\n",
    "    'outputs',\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    report_to='none'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trainer Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           num_labels=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    train_dataset=ds_dict['train'],\n",
    "    eval_dataset=ds_dict['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=pearson_correlation\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convenience Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To be able to better and more quickly experiment, let's create a function\n",
    "that quickly tokenizes and another function that creates the trainer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_ds_dict(dataframe):\n",
    "    dataset = Dataset.from_pandas(dataframe).rename_column('score', 'label')\n",
    "    tokenized_ds = dataset.map(tokenize_function, batched=True,\n",
    "                               remove_columns=inputs+('inputs', 'id',\n",
    "                                                      'section'))\n",
    "    return DatasetDict({\n",
    "        'train': tokenized_ds.select(train_indicies),\n",
    "        'valid': tokenized_ds.select(valid_indices)\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                              num_labels=1)\n",
    "\n",
    "def create_trainer(ds_dict, model=None):\n",
    "    if model is None: model = get_model()\n",
    "\n",
    "    arguments = TrainingArguments(\n",
    "        'outputs',\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        fp16=True,\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_gpu_eval_batch_size=batch_size*2,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    "    return Trainer(\n",
    "        model,\n",
    "        arguments,\n",
    "        train_dataset=ds_dict['train'],\n",
    "        eval_dataset=ds_dict['valid'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=pearson_correlation\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Improving the Model\n",
    "\n",
    "Make sure the initial model is stable; that is, it gives roughly the same\n",
    "result in each run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Separator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try using a different separator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "separator = ' [s] '\n",
    "train_df['inputs'] = train_df['context'] + separator + train_df['anchor'] + \\\n",
    "                     separator + train_df['target']\n",
    "ds_dict = create_ds_dict(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_trainer(ds_dict).train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lowercase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Changing to lowercase is often helpful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['inputs'] = train_df['inputs'].str.lower()\n",
    "ds_dict = create_ds_dict(train_df)\n",
    "create_trainer(ds_dict).train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patent Section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try making the patent section a special token. It may help the model\n",
    "recognize that different sections are to be handled in different ways."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['section token'] = '[' + train_df['section'] + ']'\n",
    "section_tokens = list(train_df['section token'].unique())\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': section_tokens})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['inputs'] = train_df['section token'] + separator + \\\n",
    "                     train_df['context'] + separator + train_df['anchor'].str\\\n",
    "                         .lower() + separator + train_df['target']\n",
    "ds_dict = create_ds_dict(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embedding matrix now needs to be resized due to the addition of more tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = create_trainer(ds_dict, model=model)\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# TODO: Use large deberta model.\n",
    "# TODO: Train on full set before submitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Credit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Iterate like a grandmaster! by Jeremy Howard](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
